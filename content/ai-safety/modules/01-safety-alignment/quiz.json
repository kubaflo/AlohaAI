{
  "moduleId": "01-safety-alignment",
  "passingScore": 70,
  "questions": [
    {
      "id": "q1",
      "type": "MultipleChoice",
      "question": "What is the purpose of harm modeling in AI safety?",
      "options": [
        "To make the model faster",
        "To systematically identify potential negative impacts before deployment",
        "To reduce training costs",
        "To improve accuracy"
      ],
      "correctIndex": 1,
      "explanation": "Harm modeling maps out ways the system could cause harm \u2014 from generating dangerous content to enabling misuse \u2014 so mitigations can be designed.",
      "xp": 5
    },
    {
      "id": "q2",
      "type": "TrueFalse",
      "question": "Red teaming involves having people deliberately try to break or misuse an AI system to find vulnerabilities.",
      "options": [
        "True",
        "False"
      ],
      "correctIndex": 0,
      "explanation": "Red teaming is adversarial testing where skilled testers try to elicit unsafe, biased, or harmful outputs to identify weaknesses before launch.",
      "xp": 5
    },
    {
      "id": "q3",
      "type": "MultipleChoice",
      "question": "A release gate in AI safety is:",
      "options": [
        "A firewall for the model",
        "A checkpoint that must be passed (safety criteria met) before deploying to users",
        "A rate limiter",
        "A user interface element"
      ],
      "correctIndex": 1,
      "explanation": "Release gates are quality/safety thresholds that must be met before a model or prompt change goes to production.",
      "xp": 5
    }
  ]
}