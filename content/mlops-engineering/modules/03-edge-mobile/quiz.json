{
  "moduleId": "03-edge-mobile",
  "passingScore": 70,
  "questions": [
    {
      "id": "q1",
      "type": "MultipleChoice",
      "question": "Model quantization reduces:",
      "options": [
        "Model accuracy only",
        "Numerical precision (e.g., FP32 to INT8) to decrease memory and increase speed",
        "Training data size",
        "The number of layers"
      ],
      "correctIndex": 1,
      "explanation": "Quantization converts model weights from high precision (32-bit float) to lower precision (8-bit int), trading minimal accuracy for major speed/size gains.",
      "xp": 5
    },
    {
      "id": "q2",
      "type": "MultipleChoice",
      "question": "Which runtime is optimized for Apple devices?",
      "options": [
        "TensorRT",
        "ONNX Runtime",
        "Core ML",
        "PyTorch"
      ],
      "correctIndex": 2,
      "explanation": "Core ML is Apple's framework optimized for running ML models on iPhone, iPad, Mac, and Apple Watch using the Neural Engine.",
      "xp": 5
    },
    {
      "id": "q3",
      "type": "TrueFalse",
      "question": "Hybrid edge-cloud inference can route simple requests to on-device models and complex ones to the cloud.",
      "options": [
        "True",
        "False"
      ],
      "correctIndex": 0,
      "explanation": "Hybrid patterns use on-device models for fast, private, simple inference and fall back to cloud models for complex tasks requiring larger models.",
      "xp": 5
    }
  ]
}