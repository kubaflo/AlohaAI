{
  "id": "02-serving-scaling",
  "title": "Serving, Scaling & Reliability",
  "description": "Run low-latency, resilient inference services at production scale",
  "lessons": [
    {
      "id": "01-inference-architectures",
      "title": "Online vs Batch Inference",
      "file": "01-inference-architectures.md",
      "xp": 10
    },
    {
      "id": "02-gpu-autoscaling",
      "title": "GPU Autoscaling & Cost Controls",
      "file": "02-gpu-autoscaling.md",
      "xp": 15
    },
    {
      "id": "03-canary-shadow",
      "title": "Canary Deploys & Shadow Testing",
      "file": "03-canary-shadow.md",
      "xp": 15
    },
    {
      "id": "04-slos-reliability",
      "title": "SLOs for AI Services",
      "file": "04-slos-reliability.md",
      "xp": 15
    }
  ],
  "quiz": "quiz.json"
}