{
  "moduleId": "04-production-vision",
  "passingScore": 70,
  "questions": [
    {
      "id": "q1",
      "type": "MultipleChoice",
      "question": "Why is GPU acceleration important for vision inference?",
      "options": [
        "GPUs are cheaper than CPUs",
        "Vision models perform massive parallel matrix operations that GPUs handle efficiently",
        "GPUs use less power",
        "GPUs have more storage"
      ],
      "correctIndex": 1,
      "explanation": "Vision models involve large tensor operations (convolutions, attention) that benefit enormously from GPU parallelism.",
      "xp": 5
    },
    {
      "id": "q2",
      "type": "TrueFalse",
      "question": "Visual domain drift occurs when production images differ significantly from training data.",
      "options": [
        "True",
        "False"
      ],
      "correctIndex": 0,
      "explanation": "Domain drift in vision happens when lighting, camera angles, image quality, or subject appearance in production diverge from the training distribution.",
      "xp": 5
    },
    {
      "id": "q3",
      "type": "MultipleChoice",
      "question": "What is a common technique for on-device vision inference?",
      "options": [
        "Using full-precision FP32 models",
        "Model quantization to INT8 or smaller",
        "Increasing model size",
        "Using cloud APIs exclusively"
      ],
      "correctIndex": 1,
      "explanation": "Quantization reduces model precision (FP32 to INT8) to fit within mobile device memory and compute constraints.",
      "xp": 5
    }
  ]
}