{
  "moduleId": "04-production-patterns",
  "passingScore": 70,
  "questions": [
    {
      "id": "q1",
      "type": "MultipleChoice",
      "question": "Semantic caching differs from exact-match caching because it:",
      "options": ["Caches raw HTTP responses", "Matches semantically similar (paraphrased) queries, not just identical ones", "Only works with OpenAI", "Is always faster"],
      "correctIndex": 1,
      "explanation": "Semantic caching uses embeddings to find similar prompts, catching paraphrased versions of the same question.",
      "xp": 5
    },
    {
      "id": "q2",
      "type": "TrueFalse",
      "question": "Streaming LLM responses primarily improves actual generation speed.",
      "options": ["True", "False"],
      "correctIndex": 1,
      "explanation": "Streaming doesn't make generation faster — it reduces perceived latency by showing tokens as they're generated instead of waiting for the full response.",
      "xp": 5
    },
    {
      "id": "q3",
      "type": "MultipleChoice",
      "question": "Property-based testing for AI features involves:",
      "options": ["Testing exact output strings", "Verifying properties/characteristics of the output rather than exact content", "Only testing in production", "Skipping tests for AI features"],
      "correctIndex": 1,
      "explanation": "Since AI outputs are non-deterministic, property-based testing verifies structural and qualitative properties (valid format, appropriate length, no PII) rather than exact strings.",
      "xp": 5
    },
    {
      "id": "q4",
      "type": "MultipleChoice",
      "question": "When an AI feature fails, the best practice is to:",
      "options": ["Show the raw API error", "Crash the app", "Degrade gracefully with a fallback response", "Silently ignore the error"],
      "correctIndex": 2,
      "explanation": "Graceful degradation shows a cached response, rule-based fallback, or a friendly error message — keeping the user experience intact.",
      "xp": 5
    }
  ]
}
